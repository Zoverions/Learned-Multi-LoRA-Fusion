{
  "name": "MoLE: Mixture of LoRA Experts",
  "description": "The official project page for MoLE, a hybrid architecture integrating Mixture-of-Experts (MoE) routing with Low-Rank Adaptation (LoRA) fusion for hierarchical multi-task language model adaptation. Includes the research paper and core codebase.",
  "requestFramePermissions": []
}