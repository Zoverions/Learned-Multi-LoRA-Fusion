
import { GoogleGenAI, Type } from "@google/genai";
import type { FusionResult } from "../types";

// FIX: Initialize GoogleGenAI client
const ai = new GoogleGenAI({apiKey: process.env.API_KEY!});

// FIX: Define response schema for structured JSON output
const responseSchema = {
  type: Type.OBJECT,
  properties: {
    finalResponse: { 
      type: Type.STRING,
      description: "The final, combined response generated by fusing the outputs from different LoRA experts."
    },
    chunks: {
      type: Type.ARRAY,
      description: "The prompt segmented into semantic chunks.",
      items: {
        type: Type.OBJECT,
        properties: {
          text: { 
            type: Type.STRING,
            description: "The text content of a single semantic chunk."
          },
          weights: {
            type: Type.OBJECT,
            description: "The fusion weights assigned to different experts for this chunk. Weights should sum to 1.0.",
            properties: {
              math: { 
                type: Type.NUMBER,
                description: "Weight for the math expert (0.0 to 1.0)."
              },
              code: { 
                type: Type.NUMBER,
                description: "Weight for the code expert (0.0 to 1.0)."
              },
              creative: { 
                type: Type.NUMBER,
                description: "Weight for the creative writing expert (0.0 to 1.0)."
              },
            },
            required: ['math', 'code', 'creative'],
          },
        },
        required: ['text', 'weights'],
      },
    },
  },
  required: ['finalResponse', 'chunks'],
};

// FIX: Implement runFusionSimulation function to call Gemini API
export async function runFusionSimulation(prompt: string): Promise<FusionResult> {
  const systemInstruction = `You are a sophisticated "Mixture of LoRA Experts" (MoLE) routing system.
Your task is to analyze a user's prompt and simulate the clause-level fusion process.

1.  **Segment the Prompt**: Break the input prompt into distinct semantic chunks. A chunk could be a sentence or a clause that focuses on a single task (e.g., a math problem, a coding request, a creative instruction).
2.  **Assign Fusion Weights**: For each chunk, determine the appropriate blend of three specialized LoRA experts: "Math", "Code", and "Creative". Assign a weight to each expert (from 0.0 to 1.0) representing its relevance to the chunk. The weights for each chunk must sum to 1.0. For example, a coding question should have a high 'code' weight, while a request for a poem should have a high 'creative' weight. A chunk with both math and creative elements might have split weights like {math: 0.6, code: 0.0, creative: 0.4}.
3.  **Generate Final Response**: Create a cohesive final response that addresses all parts of the original prompt, as if it were generated by fusing the outputs of the weighted experts.

You MUST return the output in the specified JSON format.`;

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: prompt,
    config: {
      systemInstruction,
      responseMimeType: "application/json",
      responseSchema: responseSchema,
    },
  });

  try {
    const jsonString = response.text.trim();
    const sanitizedJsonString = jsonString.replace(/^`{3}json\s*|\s*`{3}$/g, '');
    const parsedJson = JSON.parse(sanitizedJsonString);
    return parsedJson as FusionResult;
  } catch (e) {
    console.error("Failed to parse Gemini response:", e);
    console.error("Raw response text:", response.text);
    throw new Error("The model returned an invalid JSON response.");
  }
}

// FIX: Implement getLoRACombinationAnalysis function to call Gemini API
export async function getLoRACombinationAnalysis(loraNames: string[]): Promise<string> {
    if (loraNames.length === 0) {
        return "No LoRA experts selected. Select one or more experts to see an analysis.";
    }

    const prompt = `Provide a brief, expert analysis of a custom large language model created by fusing a base model with the following LoRA experts: ${loraNames.join(', ')}.

Focus on:
1.  **Synergies**: What are the potential strengths and synergistic capabilities of this combination?
2.  **Potential Weaknesses**: What are the potential drawbacks, conflicts, or blind spots? (e.g., could the 'creative' LoRA interfere with the 'code' LoRA's precision?)
3.  **Ideal Use Cases**: What types of tasks or prompts would this custom model excel at?

Keep the analysis concise, insightful, and under 100 words.`;

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: prompt,
    });

    return response.text;
}
