
import { GoogleGenAI, Type } from "@google/genai";
import type { FusionResult } from '../types';

const ai = new GoogleGenAI({ apiKey: process.env.API_KEY as string });

const responseSchema = {
  type: Type.OBJECT,
  properties: {
    chunks: {
      type: Type.ARRAY,
      description: 'An array of semantic chunks from the original prompt.',
      items: {
        type: Type.OBJECT,
        properties: {
          text: {
            type: Type.STRING,
            description: 'The text content of the chunk.',
          },
          weights: {
            type: Type.OBJECT,
            description: 'The assigned fusion weights for this chunk.',
            properties: {
              math: {
                type: Type.NUMBER,
                description: 'Weight for the Math LoRA expert (0.0 to 1.0).',
              },
              code: {
                type: Type.NUMBER,
                description: 'Weight for the Code LoRA expert (0.0 to 1.0).',
              },
              creative: {
                type: Type.NUMBER,
                description: 'Weight for the Creative Writing LoRA expert (0.0 to 1.0).',
              },
            },
            required: ['math', 'code', 'creative'],
          },
        },
        required: ['text', 'weights'],
      },
    },
    finalResponse: {
      type: Type.STRING,
      description: 'The final, coherent response generated by fusing the outputs for each chunk.',
    },
  },
  required: ['chunks', 'finalResponse'],
};


export async function runFusionSimulation(prompt: string): Promise<FusionResult> {
  const systemInstruction = `You are an AI simulating the 'Learned Multi-LoRA Fusion' system.
  Your task is to process a user's prompt by following these steps:
  1.  **Semantic Chunking**: Break the user's prompt into 1 to 3 distinct, logical, and semantic chunks. Each chunk should represent a different sub-task or idea within the prompt. This is based on the PPL-chunking method.
  2.  **Fusion Weighting**: For each chunk, act as the Sparsegen router and assign fusion weights to three specialized LoRA experts: 'math', 'code', and 'creative'. The weights for each chunk must sum to 1.0. Assign higher weights to the most relevant expert(s) for that chunk's specific task.
  3.  **Response Generation**: Generate a single, final, coherent response that fulfills all parts of the original prompt, as if you were the base model with the dynamically fused LoRAs.
  4.  **JSON Output**: Your entire output must be a single, valid JSON object that adheres to the provided schema. Do not output any text before or after the JSON object.

  Example:
  User Prompt: "What is 2 * 8? Then, write a haiku about the result."
  - Chunk 1: "What is 2 * 8?" (High 'math' weight)
  - Chunk 2: "Then, write a haiku about the result." (High 'creative' weight)
  - Final Response: "2 * 8 is 16.

  Sixteen moons arise,
  Binary stars in the sky,
  Counting up so high."
  `;

  try {
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: prompt,
      config: {
        systemInstruction: systemInstruction,
        responseMimeType: "application/json",
        responseSchema: responseSchema,
        temperature: 0.7,
      },
    });

    const jsonString = response.text.trim();
    const parsedResult = JSON.parse(jsonString);

    // Basic validation to ensure the result matches the type
    if (parsedResult && Array.isArray(parsedResult.chunks) && typeof parsedResult.finalResponse === 'string') {
        return parsedResult as FusionResult;
    } else {
        throw new Error("Invalid JSON structure received from API.");
    }
  } catch (error) {
    console.error("Error in Gemini API call:", error);
    throw new Error("Failed to simulate LoRA fusion.");
  }
}
